{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 3 - PyTorch and NumPy Neural Network",
      "provenance": [],
      "authorship_tag": "ABX9TyPrnu9OIGR+Du0pNLTzfjq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianqian/Intro-to-ML-and-DL-Using-fast.ai/blob/master/notebooks/Lesson_3_PyTorch_and_NumPy_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CByCZjvZhr0p",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the AI Academy! This is the third lesson, focused on \n",
        "introducing how to use the Python library, PyTorch, to create neural networks. The essentials of PyTorch is  extremely similar to NumPy, where Pytroch uses tensors, which is very similar to arrays. \n",
        "\n",
        "You should have already read and understood all the content from the Lesson 2 document, Deep Learning. If you would like a review on neural network structure and how neural networks are trained, watch the following videos. \n",
        "\n",
        "*   https://www.youtube.com/watch?v=bxe2T-V8XRs \n",
        "*   https://www.youtube.com/watch?v=aircAruvnKk\n",
        "\n",
        "NOTE: Educational use and distribution is permitted, but credit and attribution to AIM Academy is required. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjlg92MaIH7",
        "colab_type": "text"
      },
      "source": [
        "#Learning Objectives\n",
        "\n",
        "* Learn how tensors relate to NumPy arrays\n",
        "* Learn about torch.nn modules and the components making up code in training neural network\n",
        "* Autograd and gradient descent \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXGkA_hueMEL",
        "colab_type": "text"
      },
      "source": [
        "#What is the difference between using PyTorch and NumPy?\n",
        "PyTorch uses tensors, which are essentially arrays which can have different numbers of dimensions. PyTorch also can be run on GPUs, or graphic processing units, which allow the neural network to train faster. GPUs are often separate machines located elsewhere and is connected to your computer to train your neural network digitally through wifi.\n",
        "\n",
        "PyTorch includes automatic differentiation, which gives us the gradients for updating the weights and biases in the neural network during backpropagation. Automatic differentiation makes finding what the gradients are much easier; if we used NumPy, we would have to define the derivatives (slopes of functions) manually, but PyTorch automatically gives us the gradients by calling a method. \n",
        "\n",
        "Watch this [Khan Academy video](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) to learn more about gradients. If you do not know how to take derivatives, explore the videos located on the Introduction to Derivatives course on Khan Academy linked [here](https://www.khanacademy.org/math/calculus-all-old/taking-derivatives-calc). The following Khan Academy links are also useful to understanding derivatives and gradients:\n",
        "* [Multivariable functions](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/ways-to-represent-multivariable-functions/a/multivariable-functions)\n",
        "* [Introduction to partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives)\n",
        "* [Gradients](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4-Dv5vHZ9a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QOT_9mQkU_c",
        "colab_type": "text"
      },
      "source": [
        "How tensors are used and how to convert between numpy and tensors \n",
        "Notice how the behavior for creating NumPy arrays and PyTorch tensors are similar. When `t` and `tt` were added together, the smaller tensor `tt` was broadcasted along the y-axis of `t`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7uv_xRGhljP",
        "colab_type": "code",
        "outputId": "eb92f5ad-0b87-4058-db61-bdf9d7cfe107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "t = torch.zeros(3, 4)\n",
        "print(\"Tensor t: \\n\", t)\n",
        "\n",
        "tt = torch.tensor([3, 6, 7, 8])\n",
        "print(\"Tensor tt: \\n\", tt)\n",
        "\n",
        "print(\"Tensor t + tt: \\n\", t + tt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor t: \n",
            " tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "Tensor tt: \n",
            " tensor([3, 6, 7, 8])\n",
            "Tensor t + tt: \n",
            " tensor([[3., 6., 7., 8.],\n",
            "        [3., 6., 7., 8.],\n",
            "        [3., 6., 7., 8.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNFioMzXm959",
        "colab_type": "text"
      },
      "source": [
        "Here, we are changing the PyTorch tensor to a NumPy array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGrfHl5Zk6LH",
        "colab_type": "code",
        "outputId": "a97f907e-7314-4c3f-ab70-8b89680179f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "n = t.numpy()\n",
        "print(\"Numpy array n: \\n\", n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy array n: \n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTpugjd_m0ry",
        "colab_type": "text"
      },
      "source": [
        "Here, we are creating a NumPy array and changing it to a PyTorch tensor. Notice how PyTorch sets the data type to be a float. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxglVfo2mi6q",
        "colab_type": "code",
        "outputId": "5b1d421c-5729-4185-de61-b615f2d96566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "nn = np.ones(3)\n",
        "print(\"Numpy array nn: \\n\", nn)\n",
        "\n",
        "nt = torch.from_numpy(nn)\n",
        "print(\"Tensor nt: \\n\", nt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy array nn: \n",
            " [1. 1. 1.]\n",
            "Tensor nt: \n",
            " tensor([1., 1., 1.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_z4ffhA5RWL",
        "colab_type": "text"
      },
      "source": [
        "#Running neural networks on GPUS\n",
        "We mentioned before that neural networks are often ran on GPUs which speed up the training process. With PyTorch, the process of using the GPU is simple, as shown in this example below. The CUDA platform is a program developed by NVIDIA and allows for parallel computing on GPUs. Because CUDA is not provided on Google Colab, the code below is only in text as an example and will not work if it is ran. To learn more about CUDA and its uses, go to this [link](https://www.infoworld.com/article/3299703/what-is-cuda-parallel-programming-for-gpus.html). \n",
        "\n",
        "\n",
        "**The code below is fully credited to the PyTorch team and can be found at the documentation site, pytorch.org. The code below can be directly found at https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py.**\n",
        "```\n",
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "x = torch.empty(5, 3)\n",
        "\n",
        "# let us run this cell only if CUDA is available\n",
        "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # creates a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor called y on GPU, where device specifies the CUDA GPU \n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")`` to move an already created tensor to the GPU\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXPBm5lJ5kdI",
        "colab_type": "text"
      },
      "source": [
        "#NumPy Implementation of neural network\n",
        "Here is a NumPy implementation of a neural network with only two layers. Because NumPy does not know how to compute gradients, the gradients, or derivatives of the weights biases must be calculated manually (we have to write the code ourselves). \n",
        "\n",
        "Below, a two-layer network is created, with the randomly created input layer, and a randomly created output layer for their weights. To make this network as simple as possible, biases were not included in this network. \n",
        "\n",
        "The network will take in 64 (`N`) number of different data x and y pairs. The data is formatted into x-variables of 1000 (`D_in`) features and y-variables of 10 (`D_out`) features. In other words, the neural network will take 1000 values as inputs in order to do **ONE PREDICTION**, which is 10 output values. The neural network will do 64 (`N`) of these predictions at once. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beHkCvOLp945",
        "colab_type": "text"
      },
      "source": [
        "**The code below is fully credited to the PyTorch team and can be found at the documentation site, pytorch.org. The code below can be directly found at https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#warm-up-numpy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPQ-UiECpRfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N = 64\n",
        "D_in = 1000\n",
        "H = 100\n",
        "D_out = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVPYq6qAyvBl",
        "colab_type": "text"
      },
      "source": [
        "* Here, the input array x is created with dimensions of 64 by 1000 (`N x D_in`) \n",
        "and the target or target values are created with dimensions of 64 by 10 (`N x D_out`). \n",
        "\n",
        "* Two layers are created, `w1` and `w2`, and we will be changing the values of these arrays since these arrays are the actual **weights**. \n",
        "\n",
        "* The first layer, or weight array/matrix `w1`, has dimensions of 1000 by 100 (`D_in x H`). The size for the output of w1 can be changed, because that is the outcome for the \"inbetween\" of `w1` and `w2`. The output of `w1` will be the input for `w2`, so the weight array/matrix `w2` has dimensions 100 by 10 (`H x D_out`). \n",
        "\n",
        "* The results of `w2` are the predictions of the neural network, 10 results for each prediction. Notice how it matches the dimensions of the target array `y`, which for one prediction, has 10 features. \n",
        "\n",
        "**Final network:**\n",
        "\n",
        "* **input** `x` (64 x 1000) -> **weight** `w1` (1000 x 100) -> **weight** `w2` (100 x 10) which is also predictions\n",
        "\n",
        "These different arrays will be **matrix multiplied** together. Watch this [video](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length) about matrix multiplication, also known as dot product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rZJyPeb6Uin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "# Create random input and output data\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgpZye3k6Xfs",
        "colab_type": "text"
      },
      "source": [
        "Here, the neural network will train 500 times, as seen in the for-loop. \n",
        "\n",
        "The `learning_rate`, how much the weight matricies `w1` and `w2` will change is defined as 1e-6, or 1*10^-6. \n",
        "\n",
        "The forward pass is when the neural network uses its current weights to make a prediction. After the predictions are made, then the neural network calculates its loss, how wrong its predictions are, and changes its weights through backpropagation. ***For a more indepth review, go back and read over Lesson 2's Deep Learning document***. \n",
        "\n",
        "* Remember in NumPy, the `dot()` method indicates a matrix multiply/dot product.\n",
        "* After `x` is matrix multiplied with `w1`, the inbetween matrix `h` is created. \n",
        "* The ReLU activation function is applied on `h`, which turns any negative numbers in array `h` to 0 and leaves positive numbers the same as before. \n",
        "* `np.maximum(h, 0)` is a way to apply the ReLU activation function (it means that for each element in the resulting array, it will take the maximum of the same element postion in `h` or 0. \n",
        "* If the element in a position is negative, then that element in the new `h_relu` array will be 0, because 0 is greater than all negative numbers).\n",
        "* Finally, the `h_relu` array is matrix multiplied with the second weight matrix `w2`, resulting in `y_pred`, the final predictions. \n",
        "\n",
        "\n",
        "```\n",
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "# Forward pass: compute predicted y\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Now that we have `y_pred`, the neural netwok computes the loss. The loss used here is Mean Squared Error. For each prediction in `y_pred`, we take how far away or how \"wrong\" it is compared the actual value in `y`. We calculate how \"wrong\" the prediction is by `y_pred - y` and squaring the difference. The total loss is all the squares of the differences added together. \n",
        "\n",
        "\n",
        "```\n",
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "  # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "      print(\"Iteration: {}, Loss: {}\".format(t, loss.item()))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Finally, the weight matrices are updated during backpropagation. We won't go into too much detail about how the gradients are calculated manually, but remember that gradients are the same as the slope of a line, or how much the model will change in order to decrease the loss as much as possible. The final gradients for `w1` and `w1` are `grad_w1` and `grad_w2`. \n",
        "\n",
        "```\n",
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        " # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "```\n",
        "\n",
        "`w1` and `w2` are updated by **subtracting the learning rate * gradient.** Remember the learning rate is by how much the weight matrices `w1` and `w2` change. It's important to not have a too large or too small learning rate. \n",
        "\n",
        "```\n",
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        " # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "```\n",
        "Try running the code below. Notice that as the iterations keep going, the loss eventually decreases to almost 0. That means the model is lowering its loss based on updating the gradient. The loss is printed for every 100 iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZvefrWS6Uqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "      print(\"Iteration: {}, Loss: {}\".format(t, loss.item()))\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAoxT2c_5qcn",
        "colab_type": "text"
      },
      "source": [
        "#PyTorch implementation of neural network\n",
        "Below, a Pytorch implementation of a neural network is used. Notice how. Notice how tensors are used instead of NumPy arrays. Tensors are are arrays with different number of dimensions, but they can also calculate the gradient without requiring us to manually code how to calculate the gradients.\n",
        "\n",
        "Exactly same to the NumPy neural network above, the input array x and target value array y are created with the following dimensions. \n",
        "\n",
        "**input array** `x` (64 x 1000) `N x D_in`\n",
        "\n",
        "**target value array** `y` (64 x 10) `N x D_out`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lB8IPEoW5NYE"
      },
      "source": [
        "**The code below is fully credited to the PyTorch team and can be found at the documentation site, pytorch.org. The code below can be directly found at https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html#sphx-glr-beginner-examples-nn-two-layer-net-optim-py.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnSHCSIpLZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "#USE# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N = 64\n",
        "D_in = 1000\n",
        "H = 100\n",
        "D_out = 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "livYqEq34wcY",
        "colab_type": "text"
      },
      "source": [
        "* Here, the network is composed of a class of layers compared to pure arrays. The `nn` package is included in the PyTorch library, and defines layers as `Modules`. \n",
        "\n",
        "* The model contains two linear layers, which take an input Tensor and output a Tensor after applying a matrix multiply with its own weight matrices. Notice how the dimensions for the `nn.Sequential` layers are the same as `w1` and `w2` in the NumPy neural network. In between the two `nn.Sequential` layers, a `nn.ReLU` activation function is applied to the inbetween outputted matrix. \n",
        "\n",
        "* The `nn` package also includes how to compute the loss. The `loss_fn` function, defined by `nn.MSELoss`, will allow us to pass in the final predicted y-values and the actual target y-values and give us the total loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D_2QUDE5Igv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IyXwvMQ40jg",
        "colab_type": "text"
      },
      "source": [
        "PyTorch also includes an `optim` package, which allows the model to control the gradients of the weights without being manually told to do so. **Optimizers are different ways to \"optimize\" or make the algorithm for decreasing the loss as efficient, or as fast, as possible.** Read this link for more information about [optimizers](https://algorithmia.com/blog/introduction-to-optimizers). \n",
        "\n",
        "The `optim.SGD` optimizer applies **stochastic gradient descent**, which means that the loss is calculated over the entire batch; the loss is based off of all 64 elements passed into the neural network. The total loss is the sum of the individual 64 losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUM_9f4Y5I_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use Adam; the optim package contains many other\n",
        "# optimization algoriths. The first argument to the Adam constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEfD9xW4nZS",
        "colab_type": "text"
      },
      "source": [
        "Similar to the previous NumPy neural network, this loop is where the model trains for 500 iterations. \n",
        "\n",
        "* `y_pred` is calculated by calling the model that we created in previous code blocks, and the input array `x` is passed into the model. \n",
        "\n",
        "* Instead of calculating the loss manually, `y_pred` and `y` are passed into `loss_fn` which was also created in the code blocks above. (The loss is printed for every 100 iterations.)\n",
        "\n",
        "* When `optimizer.zero_grad()` is called, the **gradients are set to 0, or rather reset.** \n",
        " * Every time the neural network calculates the final prediction, the gradients need to be **calculated only based on that one iteration**, not several iterations over and over (which is what will happen if `optimizer.zero_grad()` is not called). \n",
        "\n",
        "* When `loss.backward()` is called, the model **calculates the gradient with respect to its previous calculations with Tensors from the `loss_fn`.** \n",
        "  * The `autograd` package is what PyTorch uses to calculate these gradients esaily through automatic differentiation. \n",
        "  *Notice how this only takes 1 line, as compared to 6 lines when using the NumPy library. \n",
        "\n",
        "* When `loss.backward()` is called, the gradients are **calculated based on the calculations applied by the weight matricies inside the model.** \n",
        " * All the Tensors inside the model have `requires_grad=True`, which is a parameter that we can set in order to calculate gradients automatically. \n",
        "\n",
        "* Where are the gradients stored? \n",
        " * The weights themselves can be accessed through the model by calling `model.parameters()`, and the gradients are stored for each layer in the `.grad` attribute. \n",
        " * We multiply the learning rate by the gradients and update the model parameters that way. \n",
        " * Because we used the optimizers, **all of these can be completed by calling `optimizer.step()`, which removes the need to manually update the weights.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyx0RB4P5IC-",
        "colab_type": "code",
        "outputId": "7026e6a0-2192-427a-dcf7-8ee4891710c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#CODE BELOW IS FULLY CREDITED TO PYTORCH (pytorch.org)\n",
        "#USED ONLY FOR EDUCATIONAL PURPOSES UNDER FAIR USE\n",
        "\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(\"Iteration: {}, Loss: {}\".format(t, loss.item()))\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call param.grad for param in model.parameters() will be Tensors holding the gradient\n",
        "    # of the loss with respect to the model parameters. \n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 99, Loss: 2.3015968799591064\n",
            "Iteration: 199, Loss: 0.03412647545337677\n",
            "Iteration: 299, Loss: 0.001003907178528607\n",
            "Iteration: 399, Loss: 4.040964267915115e-05\n",
            "Iteration: 499, Loss: 2.016114422076498e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4vaLFZ-uiru",
        "colab_type": "text"
      },
      "source": [
        "Notice if you keep running the above code cell, the same model or weights are kept for each layer, and the loss continues to decrease. \n",
        "\n",
        "Once you have gone through this notebook once, go through it again to make sure you understand why the code works the way it is. If you have any questions, do not be afraid to ask. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqjhSPV0dfS-",
        "colab_type": "text"
      },
      "source": [
        "#Using .backward()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBlPXDETLiAV",
        "colab_type": "code",
        "outputId": "893330de-b98b-4ff1-9ae9-f9392d977c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import torch\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x * x\n",
        "z = y.mean()\n",
        "z.backward() # computes the gradients \n",
        "print(x.grad) # where the gradient is being stored "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[0.5000, 0.5000],\n",
            "        [0.5000, 0.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}